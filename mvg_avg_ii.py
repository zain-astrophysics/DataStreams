# -*- coding: utf-8 -*-
"""MVG_avg_II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rZftFAE-Csn9YhMeCQSAhF53T5D9qgT5
"""

import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.streaming import GroupState, GroupStateTimeout
import sys, time
# from google.colab import drive
import pyspark
from pyspark.conf import SparkConf
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col, avg, when, window
from pyspark.sql.window import Window
from pyspark.sql.types import TimestampType


def setLogLevel(sc, level):
    from pyspark.sql import SparkSession
    spark = SparkSession(sc)
    spark.sparkContext.setLogLevel(level)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: stream_twelvedata.py <hostname> <port>", file=sys.stderr)
        sys.exit(-1)

    print('Argv', sys.argv)
    host = sys.argv[1]
    port = int(sys.argv[2])
    print('host', type(host), host, 'port', type(port), port)

    sc_bak = SparkContext.getOrCreate()
    sc_bak.stop()
    time.sleep(15)
    print('Ready to work!')

    ctx = pyspark.SparkContext(appName="stock_data", master="local[*]")
    print('Context', ctx)

    spark = SparkSession(ctx).builder.getOrCreate()
    sc = spark.sparkContext

    setLogLevel(sc, "WARN")
    print('Session:', spark)
    print('SparkContext', sc)

    # Create DataFrame representing the stream of input lines from connection to host:port
    data = spark \
        .readStream \
        .format('socket') \
        .option('host', host) \
        .option('port', port) \
        .load()

    # Parse the data into columns
    stock = data.select(
        split(data.value, ' ').getItem(0).alias('DateTime'),
        split(data.value, ' ').getItem(1).alias('Symbol'),
        split(data.value, ' ').getItem(2).cast('float').alias('Price')
    )



    # Convert Datetime column to a proper timestamp type (if it's not already)
    stock = stock.withColumn("Datetime", col("DateTime").cast(TimestampType() ))


    # Filter for AAPL stock data
    aaplPrice = stock.filter(col("Symbol") == "AAPL")

# Assume `aaplPrice` is your streaming DataFrame with columns: "Datetime" and "Price", and "Stock"
# (for this example, all rows have Stock = "AAPL", but the approach works for keyed streams)

def update_moving_average(stock, rows, state: GroupState):
    """
    For each stock symbol, update the state's buffer with new rows,
    discard records older than 10 days from the latest timestamp,
    and compute the 10-day moving average.

    Parameters:
      stock: key for the group (e.g., "AAPL")
      rows: iterator over rows in the current micro-batch for that key.
      state: GroupState to maintain the buffer of previous records.

    Yields:
      A tuple: (stock, current_time, 10_day_MA)
    """
    # Retrieve the existing state (buffer) if it exists; otherwise, initialize an empty list.
    buffer = state.get("buffer") if state.exists else []

    # Extend the buffer with new rows
    for row in rows:
        # Each row is expected to have attributes "Datetime" (as Python datetime) and "Price"
        buffer.append((row.Datetime, row.Price))

    # Ensure the buffer is sorted by datetime (oldest first)
    buffer.sort(key=lambda x: x[0])

    # Use the most recent timestamp in the buffer as the current time
    current_time = buffer[-1][0]
    cutoff = current_time - datetime.timedelta(days=10)

    # Remove records older than 10 days compared to current_time
    buffer = [record for record in buffer if record[0] >= cutoff]

    # Compute the 10-day moving average
    avg_price = sum(record[1] for record in buffer) / len(buffer)

    # Update the state with the pruned buffer
    state.update({"buffer": buffer})

    # Yield a tuple with (Stock, current timestamp, moving average)
    yield (stock, current_time, avg_price)

# In order to use mapGroupsWithState, we first key the stream by "Stock".
# In PySpark, the API is available via groupByKey().
# Note: mapGroupsWithState returns a Dataset of the stateful results.
statefulMA = (
    aaplPrice
    .groupByKey(lambda row: row.Stock)
    .flatMapGroupsWithState(
        func=update_moving_average,
        outputMode="update",  # this indicates we output a result each time the state is updated
        timeoutConf=GroupStateTimeout.NoTimeout
    )
)

# Define a schema for converting the stateful output to a DataFrame (if needed)
resultDF = statefulMA.toDF(["Stock", "Datetime", "10_day_MA"])

# Write the results to the console for debugging, or use another sink as required.
query = (
    resultDF.writeStream
    .outputMode("update")
    .format("console")
    .start()
)

query.awaitTermination()