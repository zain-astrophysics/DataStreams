# -*- coding: utf-8 -*-
"""MVG_avg_II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rZftFAE-Csn9YhMeCQSAhF53T5D9qgT5
"""

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp
import sys, time
# from google.colab import drive
import pyspark
from pyspark.conf import SparkConf
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col, avg, when, window
from pyspark.sql.window import Window
from pyspark.sql.types import TimestampType


def setLogLevel(sc, level):
    from pyspark.sql import SparkSession
    spark = SparkSession(sc)
    spark.sparkContext.setLogLevel(level)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: stream_twelvedata.py <hostname> <port>", file=sys.stderr)
        sys.exit(-1)

    print('Argv', sys.argv)
    host = sys.argv[1]
    port = int(sys.argv[2])
    print('host', type(host), host, 'port', type(port), port)

    sc_bak = SparkContext.getOrCreate()
    sc_bak.stop()
    time.sleep(15)
    print('Ready to work!')

    ctx = pyspark.SparkContext(appName="stock_data", master="local[*]")
    print('Context', ctx)

    spark = SparkSession(ctx).builder.getOrCreate()
    sc = spark.sparkContext

    setLogLevel(sc, "WARN")
    print('Session:', spark)
    print('SparkContext', sc)

    # Create DataFrame representing the stream of input lines from connection to host:port
    data = spark \
        .readStream \
        .format('socket') \
        .option('host', host) \
        .option('port', port) \
        .load()

    # Parse the data into columns
    stock = data.select(
        split(data.value, ' ').getItem(0).alias('DateTime'),
        split(data.value, ' ').getItem(1).alias('Symbol'),
        split(data.value, ' ').getItem(2).cast('float').alias('Price')
    )



    # Convert Datetime column to a proper timestamp type (if it's not already)
    stock = stock.withColumn("Datetime", col("DateTime").cast(TimestampType() ))


    # Filter for AAPL stock data
    aaplPrice = stock.filter(col("Symbol") == "AAPL")

# Global state (this will be maintained in the driver for demonstration).
# In production, use an external state store.
global_state = None

def process_batch(batch_df, batch_id):
    """
    This function is called for every micro-batch.
    It:
       1. Converts the batch Spark DataFrame to a Pandas DataFrame.
       2. Merges it with historical (global) data.
       3. Prunes records older than 10 days from the current maximum timestamp.
       4. Calculates the rolling 10 Day Moving Average using a time-based window.
       5. Prints out the latest computed values.
    """
    global global_state
    import pandas as pd

    # Check if the batch is empty.
    if batch_df.rdd.isEmpty():
        return

    # Convert the current batch to a Pandas DataFrame.
    batch_pdf = batch_df.toPandas()
    batch_pdf['datetime'] = pd.to_datetime(batch_pdf['datetime'])
    batch_pdf.sort_values('datetime', inplace=True)

    # Merge with previous state.
    if global_state is None:
        global_state = batch_pdf
    else:
        global_state = pd.concat([global_state, batch_pdf], ignore_index=True)

    # Sort the combined data by datetime.
    global_state.sort_values('datetime', inplace=True)

    # Determine the latest timestamp and calculate the cutoff (10 days ago).
    current_time = global_state['datetime'].max()
    cutoff = current_time - pd.Timedelta(days=10)

    # Prune data older than 10 days.
    global_state = global_state[global_state['datetime'] >= cutoff]

    # Use a time-based rolling window to calculate the 10-day moving average.
    # Set 'datetime' as the index for the rolling operation.
    df_indexed = global_state.set_index('datetime')
    df_indexed['10_day_MA'] = df_indexed['close'].rolling('10D', min_periods=1).mean()

    # For demonstration, print the last few rows with the moving average.
    print(f"\nBatch {batch_id} - Latest 10 Day Moving Average:")
    print(df_indexed.tail(3))

    # Optionally, you can write the result to an external sink (database, file, etc.)
    # and update the global state accordingly (ensuring state is persisted reliably).

# Configure the streaming query with the foreachBatch sink.
query = aaplPrice.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .start()

query.awaitTermination()



