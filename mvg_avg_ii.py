# -*- coding: utf-8 -*-
"""MVG_avg_II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rZftFAE-Csn9YhMeCQSAhF53T5D9qgT5
"""

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp
import sys, time
# from google.colab import drive
import pyspark
from pyspark.conf import SparkConf
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col, avg, when, window
from pyspark.sql.window import Window
from pyspark.sql.types import TimestampType


def setLogLevel(sc, level):
    from pyspark.sql import SparkSession
    spark = SparkSession(sc)
    spark.sparkContext.setLogLevel(level)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: stock_feeder.py <hostname> <port>", file=sys.stderr)
        sys.exit(-1)

    print('Argv', sys.argv)
    host = sys.argv[1]
    port = int(sys.argv[2])
    print('host', type(host), host, 'port', type(port), port)

    sc_bak = SparkContext.getOrCreate()
    sc_bak.stop()
    time.sleep(15)
    print('Ready to work!')

    ctx = pyspark.SparkContext(appName="stock_data", master="local[*]")
    print('Context', ctx)

    spark = SparkSession(ctx).builder.getOrCreate()
    sc = spark.sparkContext

    setLogLevel(sc, "WARN")
    print('Session:', spark)
    print('SparkContext', sc)

    # Create DataFrame representing the stream of input lines from connection to host:port
    data = spark \
        .readStream \
        .format('socket') \
        .option('host', host) \
        .option('port', port) \
        .load()

    # Parse the data into columns
    stock = data.select(
        split(data.value, ' ').getItem(0).alias('DateTime'),
        split(data.value, ' ').getItem(1).alias('Symbol'),
        split(data.value, ' ').getItem(2).cast('float').alias('Price')
    )



    # Convert Datetime column to a proper timestamp type (if it's not already)
    stock = stock.withColumn("Datetime", col("DateTime").cast(TimestampType() ))


    # Filter for AAPL stock data
    # aaplPrice = stock.filter(col("Symbol") == "AAPL")

 # Define a window specification to compute a 10-day moving average
    window_spec = Window.partitionBy('Symbol').orderBy('Datetime').rowsBetween(-9, 0)

    # Filter for AAPL stock and compute the 10-day moving average
    aaplPrice = stock.filter(col("Symbol") == "AAPL").withColumn("10DayMA", avg("Price").over(window_spec))

    # Filter for MSFT stock and compute the 10-day moving average
    # msftPrice = stock.filter(col("Symbol") == "MSFT").withColumn("10DayMA", avg("Price").over(window_spec))

    # Write the results to the console (for testing purposes)
    aaplPrice.writeStream \
        .outputMode("append") \
        .format("console") \
        .start()

    # msftPrice.writeStream \
        # .outputMode("append") \
        # .format("console") \
        # .start()

    # Wait for the streaming queries to finish
    spark.streams.awaitAnyTermination()



